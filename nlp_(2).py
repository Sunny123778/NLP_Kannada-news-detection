# -*- coding: utf-8 -*-
"""NLP (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BmljI_FNfDkkyQe6JpBSQh0N7P-XBcpt
"""

from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt # plotting
import pandas as pd
import numpy as np
import os
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('nlp.csv')

# Check the first few rows of the dataset
print(df.head(5))

df_train = pd.read_csv('nlp.csv', header=None)
df_train.head()

import re
import string
import demoji


# Define the tokenize function for Kannada language
def tokenize(text, language):
    # Tokenize Kannada text
    tokens = text.split()  # This is a simple example, you may need more sophisticated tokenization
    return tokens

# Define the remove_stopwords function for Kannada language
def remove_stopwords(tokens, language):
    # Define your logic to remove stopwords for Kannada language
    stopwords = [...]  # Define your list of Kannada stopwords
    filtered_tokens = [token for token in tokens if token not in stopwords]
    return filtered_tokens

def clean_text(text):
    # Remove Hashtag, Mention, URLs
    pattern = re.compile(r"(#[\u0C80-\u0CFF]+|@[A-Za-z0-9]+|https?://\S+|www\.\S+|\S+\.[a-z]+|RT @)")
    text = pattern.sub('', text)
    text = " ".join(text.split())

    # Make all text lowercase
    text = text.lower()

    # Tokenize text
    tokens = tokenize(text, 'kn')

    # Remove stopwords
    tokens = remove_stopwords(tokens, 'kn')

    # Join tokens back into text
    cleaned_text = ' '.join(tokens)

    # Removing Punctuations
    remove_punc = re.compile(r"[%s]" % re.escape(string.punctuation))
    cleaned_text = remove_punc.sub('', cleaned_text)

    # Taking care of emojis
    emoji = demoji.findall(cleaned_text)
    for emot in emoji:
        cleaned_text = re.sub(r"(%s)" % (emot), "_".join(emoji[emot].split()), cleaned_text)

    return cleaned_text

# Apply the clean_text function to the Kannada dataset
df['cleaned_text'] = df['headline'].apply(clean_text)

pip install demoji

print(df.head())

df['cleaned_text'].duplicated().sum() # Checking for duplicate values

df = df[df["headline"]!="Main_Label"]

data.shape

df['Main_Label'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df, x='Main_Label')
plt.show()

import plotly.express as px
for cyber_type in df.Main_Label.unique():

    top50_word = df.cleaned_text[df.Main_Label==cyber_type].str.split(expand=True).stack().value_counts()[:15]

    fig = px.bar(top50_word, color=top50_word.values, color_continuous_scale=px.colors.sequential.RdPu, custom_data=[top50_word.values])
    fig.update_traces(marker_color='brown')
    fig.update_traces(hovertemplate='<b>Count: </b>%{customdata[0]}')
    fig.update_layout(title=f"Top 15 words for {cyber_type}",
                     template='simple_white',
                     hovermode='x unified')
    fig.show()

import re
import string
import demoji

# Define the tokenize function for Kannada language
def tokenize(text, language):
    # Tokenize Kannada text
    tokens = text.split()  # This is a simple example, you may need more sophisticated tokenization
    return tokens

# Define the remove_stopwords function for Kannada language
def remove_stopwords(tokens, language):
    # Define your logic to remove stopwords for Kannada language
    stopwords = [...]  # Define your list of Kannada stopwords
    filtered_tokens = [token for token in tokens if token not in stopwords]
    return filtered_tokens

def clean_text(text):
    # Remove Hashtag, Mention, URLs
    pattern = re.compile(r"(#[\u0C80-\u0CFF]+|@[A-Za-z0-9]+|https?://\S+|www\.\S+|\S+\.[a-z]+|RT @)")
    text = pattern.sub('', text)
    text = " ".join(text.split())

    # Make all text lowercase
    text = text.lower()

    # Tokenize text
    tokens = tokenize(text, 'kn')

    # Remove stopwords
    tokens = remove_stopwords(tokens, 'kn')

    # Join tokens back into text
    cleaned_text = ' '.join(tokens)

    # Removing Punctuations
    remove_punc = re.compile(r"[%s]" % re.escape(string.punctuation))
    cleaned_text = remove_punc.sub('', cleaned_text)

    # Taking care of emojis
    emoji = demoji.findall(cleaned_text)
    for emot in emoji:
        cleaned_text = re.sub(r"(%s)" % (emot), "_".join(emoji[emot].split()), cleaned_text)

    return cleaned_text

# Apply the clean_text function to the Kannada dataset
df['cleaned_text'] = df['headline'].apply(clean_text)

print(df.head())
df['cleaned_text'].duplicated().sum() # Checking for duplicate values
df = df[df["headline"]!="Main_Label"]
df['Main_Label'].value_counts()

X = df['cleaned_text']  # Feature (raw data)
y = df['Main_Label']  # Target Label

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

tfidf = TfidfVectorizer(max_features = 5000)

X_train_tfidf = tfidf.fit_transform(X_train)  # Creating the vocabulary only from the training set to avoid data leakage from
X_test_tfidf = tfidf.transform(X_test)

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import RandomOverSampler, ADASYN
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Load your Kannada dataset into a DataFrame
df = pd.read_csv('nlp.csv')

# Assuming 'features' are your independent variables and 'target' is your dependent variable
X = df.drop('headline', axis=1)  # Features
y = df['Main_Label']  # Target

# Step 2: Encode labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Step 3: Encode categorical features in X using one-hot encoding
categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(exclude=['object']).columns

preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

X_encoded = preprocessor.fit_transform(X)

# Step 4: Apply RandomOverSampler for oversampling
ros = RandomOverSampler(sampling_strategy='minority', random_state=42)
X_resampled_ros, y_resampled_ros = ros.fit_resample(X_encoded, y_encoded)

# Check the shape of the oversampled data
print("Shape after RandomOverSampler:", X_resampled_ros.shape)


# Step 6: Plot the count of labels after oversampling
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
unique_labels_ros, label_counts_ros = np.unique(y_resampled_ros, return_counts=True)
plt.bar(label_encoder.inverse_transform(unique_labels_ros), label_counts_ros)
plt.title("Count of Labels after RandomOverSampler")
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=45)


plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
# Step 7: Split the oversampled data into training and testing sets
X_train_ros, X_test_ros, y_train_ros, y_test_ros = train_test_split(X_resampled_ros, y_resampled_ros, test_size=0.3, random_state=42)

# Step 8: Train a model on the oversampled data (using RandomOverSampler)
clf_ros = RandomForestClassifier(random_state=42)
clf_ros.fit(X_train_ros, y_train_ros)
y_pred_ros = clf_ros.predict(X_test_ros)
print("Classification Report for RandomOverSampler:")
print(classification_report(y_test_ros, y_pred_ros))

# Cross-validation to check for overfitting
cv_scores_ros = cross_val_score(clf_ros, X_resampled_ros, y_resampled_ros, cv=5)
print("Cross-validation scores for RandomOverSampler:", cv_scores_ros)

# Step 8: Train a model on the oversampled data (using RandomOverSampler)
from sklearn.svm import SVC
from sklearn.metrics import classification_report
clf_ros_svm = SVC(random_state=42)
clf_ros_svm.fit(X_train_ros, y_train_ros)
y_pred_ros_svm = clf_ros_svm.predict(X_test_ros)
print("Classification Report for RandomOverSampler with SVM:")
print(classification_report(y_test_ros, y_pred_ros_svm))

from imblearn.over_sampling import RandomOverSampler
from matplotlib import pyplot as plt
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical

# Step 1: Load your Kannada dataset into a DataFrame
df = pd.read_csv('nlp.csv')

# Assuming 'features' are your independent variables and 'target' is your dependent variable
X = df.drop('headline', axis=1)  # Features
y = df['Main_Label']  # Target

# Step 2: Encode labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
y_categorical = to_categorical(y_encoded)

# Step 3: Apply RandomOverSampler for oversampling
oversampler = RandomOverSampler(sampling_strategy="minority")
oversampled_data, oversampled_target = oversampler.fit_resample(X, y_categorical)

# Step 4: Check the shape of the oversampled data
print(oversampled_data.shape)

# Plot the count of labels after oversampling
plt.figure(figsize=(10, 6))
plt.bar(label_encoder.classes_, oversampled_target.sum(axis=0))
plt.title("Count of Labels after Oversampling")
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

X_train_tfidf

X_test_tfidf

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
tfidf_array_train = X_train_tfidf.toarray()   # Converting the sparse matrix to a numpy array (dense matrix)
tfidf_array_test = X_test_tfidf.toarray()     # Converting the sparse matrix to a numpy array (dense matrix)
scaled_X_train = scaler.fit_transform(tfidf_array_train)  # Fitting on only training data to avoid data leakage from test data
scaled_X_test = scaler.transform(tfidf_array_test) # and then tranforming both training and testing data

from sklearn.decomposition import PCA

# Choose a value for n_components within the valid range
# For example, you could set it to min(n_samples, n_features)
NUM_COMPONENTS = min(scaled_X_train.shape[0], scaled_X_train.shape[1])

pca = PCA(n_components=NUM_COMPONENTS)
reduced = pca.fit_transform(scaled_X_train)

variance_explained = np.cumsum(pca.explained_variance_ratio_)  # Calculating the cumulative explained variance by the components

# Plotting
fig, ax = plt.subplots(figsize=(8, 6))
plt.plot(range(NUM_COMPONENTS),variance_explained, color='r')
ax.grid(True)
plt.xlabel("Number of components")
plt.ylabel("Cumulative explained variance")

final_pca = PCA(0.9)
reduced_90 = final_pca.fit_transform(scaled_X_train) # Number of Components explaining 90% variance in the training data

reduced_90_test = final_pca.transform(scaled_X_test)

reduced_90.shape

## Model Training
import sklearn
print(sklearn.__version__)

!pip install scikit-learn

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# LOGISTIC REGRESSION with the complete data
from sklearn.linear_model import LogisticRegression
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV
log_model_pca = LogisticRegression()
log_model_pca.fit(reduced_90, y_train)
preds_log_model_pca = log_model_pca.predict(reduced_90_test)
print(classification_report(y_test, preds_log_model_pca))
log_model = LogisticRegression(solver = 'saga')
param_grid = {'C': np.logspace(0, 10, 5)}
grid_log_model = HalvingGridSearchCV(log_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)
grid_log_model.fit(X_train_tfidf, y_train)
preds_grid_log_model = grid_log_model.predict(X_test_tfidf)
print(y_test, preds_grid_log_model)

# SUPPORT VECTOR MACHINES
from sklearn.svm import LinearSVC
svm_model = LinearSVC()
C = [1e-5, 1e-4, 1e-2, 1e-1, 1]
param_grid = {'C': C}
grid_svm_model = HalvingGridSearchCV(svm_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)
grid_svm_model.fit(X_train_tfidf, y_train)
preds_grid_svm_model = grid_svm_model.predict(X_test_tfidf)
print(classification_report(y_test, preds_grid_svm_model))

# RANDOM FORESTS
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(random_state = 42)
n_estimators = [64, 100, 128]
bootstrap = [True, False] # Bootstrapping is true by default
param_grid = {'n_estimators': n_estimators, 'bootstrap': bootstrap}
grid_rf_model = HalvingGridSearchCV(rf_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)
grid_rf_model.fit(X_train_tfidf, y_train)
preds_grid_rf_model = grid_rf_model.predict(X_test_tfidf)
print(classification_report(y_test, preds_grid_rf_model))

# NEURAL NETWORKS
from sklearn.neural_network import MLPClassifier
nn_model = MLPClassifier(activation = 'logistic', max_iter = 10)  # Sigmoid Activation Function
param_grid = {'learning_rate_init': [0.001, 0.0015, 0.002, 0.0025]}
grid_nn_model = HalvingGridSearchCV(nn_model, param_grid = param_grid, n_jobs = -1, min_resources = 'exhaust', factor = 3)
grid_nn_model.fit(X_train_tfidf, y_train)
preds_grid_nn_model = grid_nn_model.predict(X_test_tfidf)
print(classification_report(y_test, preds_grid_nn_model))

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import HalvingGridSearchCV
from sklearn.metrics import classification_report

# Define base estimator
base_estimator = DecisionTreeClassifier()

# Initialize AdaBoostClassifier
adaboost_model = AdaBoostClassifier(base_estimator=base_estimator)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.1, 0.5, 1.0]
}

# Initialize HalvingGridSearchCV
grid_adaboost_model = HalvingGridSearchCV(adaboost_model, param_grid=param_grid, n_jobs=-1, min_resources='exhaust', factor=3)

# Fit the model
grid_adaboost_model.fit(X_train_tfidf, y_train)
# Make predictions
preds_grid_adaboost_model = grid_adaboost_model.predict(X_test_tfidf)

# Print classification report
print(classification_report(y_test, preds_grid_adaboost_model))